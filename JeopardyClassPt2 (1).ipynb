{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow_hub) (6.33.1)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (10.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\abiga\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow_hub) (0.1.0)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras, tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.16.1 tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1uZZoCs4LuL",
    "outputId": "7c3cf1a3-1bdf-450e-8bf0-9ae549b955f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abiga\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\abiga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abiga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abiga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf_text\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab') #for some reason I needed all of these downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JwHArDgB7hkz"
   },
   "outputs": [],
   "source": [
    "DIRNAME = 'jeopardy.json'\n",
    "english_stopwords = set(stopwords.words('english') + list(punctuation) + ['..','...','....','``',\"''\", '//n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zw-lBvYs6lWW"
   },
   "outputs": [],
   "source": [
    "jepDict = json.load(open(DIRNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dwCJF4Yd6_fN"
   },
   "outputs": [],
   "source": [
    "for entry in jepDict:\n",
    "  values = entry.get('value', '')\n",
    "  if values:\n",
    "    cleanVals = values.replace(',','').replace('$', '')\n",
    "    value = int(cleanVals)\n",
    "  elif values == 'None':\n",
    "    value = 0\n",
    "  entry['value'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tcAx3QJ7qD1v"
   },
   "outputs": [],
   "source": [
    "def clean_wordlist(jsonList):\n",
    "  '''\n",
    "  Takes a list of dictionaries, and provides\n",
    "  a word list for all the questions and answers, lemmatized wihout stopwwords.\n",
    "  '''\n",
    "  corpus = ''\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for dictionary in jsonList:\n",
    "    question = dictionary.get('question').lower()\n",
    "    answer = dictionary.get('answer').lower()\n",
    "    corpus += '\\n'.join([question, answer])\n",
    "  wordlist = [lemmatizer.lemmatize(x) for x in word_tokenize(corpus) if x not in english_stopwords]\n",
    "  return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "05yfDCCjq2tV"
   },
   "outputs": [],
   "source": [
    "jepWords = str(clean_wordlist(jepDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jOfIyawJrIOl"
   },
   "outputs": [],
   "source": [
    "hard_text = [entry for entry in jepDict if entry['value'] > 600]\n",
    "hardWords = set(clean_wordlist(hard_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rzf_6xDKrKfU"
   },
   "outputs": [],
   "source": [
    "labels = [ 1 if word in hardWords else 0 for word in jepWords]\n",
    "\n",
    "df = pd.pandas.DataFrame({'text':jepWords, 'hard':labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAG9PhDv8qbj"
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing or NaN values\n",
    "df = df.dropna(subset=['text', 'hard'])\n",
    "\n",
    "# Convert all words to strings\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Strip whitespace and lowercase (optional but recommended)\n",
    "df['text'] = df['text'].str.strip().str.lower()\n",
    "\n",
    "# Remove empty strings\n",
    "df = df[df['text'] != \"\"]\n",
    "\n",
    "# Ensure labels are integers (0/1)\n",
    "df['hard'] = df['hard'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "9JrJp6QLrWsL"
   },
   "outputs": [],
   "source": [
    "features = df['text'].to_numpy()\n",
    "labels = df['hard'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQ72iKt9DiO3",
    "outputId": "57ad131f-479e-459c-cd18-c0c0f1f1a8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2280404\n",
      "First 10 features: [\"'for\", 'last', '8', 'year', 'life', 'galileo', 'house', 'arrest', 'espousing', 'man']\n",
      "First 10 labels: [np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "# Convert everything to string and strip whitespace\n",
    "features_clean = [str(w).strip() for w in features if str(w).strip() != \"\"]\n",
    "\n",
    "labels_clean = [labels[i] for i, w in enumerate(features) if str(w).strip() != \"\"]\n",
    "\n",
    "# Sanity check\n",
    "print(\"Number of samples:\", len(features_clean))\n",
    "print(\"First 10 features:\", features_clean[:10])\n",
    "print(\"First 10 labels:\", labels_clean[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmXoZJo2OnD4",
    "outputId": "ed3b15d1-809c-481f-df9f-e7d9ae9bd9ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "train, val, test = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "v1_rAJKDO4RU"
   },
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=10000):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop('hard')\n",
    "    df = df['text']\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tbg0VRNQP6Qz"
   },
   "outputs": [],
   "source": [
    "train_data = df_to_dataset(train)\n",
    "valid_data = df_to_dataset(val)\n",
    "test_data = df_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSuGNonEWAlw"
   },
   "source": [
    "Using a model that is already trained that just needs a few tweaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azEj45yAU3Tl"
   },
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqAqfsI-WfNg"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activationn = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd0TF7L7YJ0P"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbqyQozBYh9z"
   },
   "outputs": [],
   "source": [
    "model.evaluate(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhmXJh-bYlKJ"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7DEcfynYp9X"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=10, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1mwbYZtWwPF"
   },
   "source": [
    "I honestly do not know if this is actually going to work. Colab RAM started skyrocketing and I could not for the life of me install what was needed on jupyter. I am tired, I have worked on this for 10+ hours today and restarted at hour 8. I watched this video and my sessions would not get past my cleaning section before it crashed. https://www.bing.com/videos/riverview/relatedvideo?q=binary+text+classification+tensorflow+tutorial+for+beginners&&view=riverview&mmscn=mtsc&mid=AE3CDD1455739689F86AAE3CDD1455739689F86A&&aps=1142&FORM=VMSOVR So the model training and splitting is mainly just copied from what I watched, applied to my already cleaned data. She goes into overfitting in the video but I am skipping over that for now. I really liked this video, I wish I knew how it worked out with this data, but I though that she did a great job of explaining things and know a lot more about TensorFlow now."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
