{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RxFFOwkNyhp"
   },
   "source": [
    "First we need to import all of the modules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UW9x4YOKMQFl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\abiga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abiga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abiga\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab') #for some reason I needed all of these downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "#Setting a dirname to make the program easier for other people to use\n",
    "DIRNAME = 'jeopardy.json'\n",
    "#initializes the stopwords\n",
    "english_stopwords = set(stopwords.words('english') + list(punctuation) + ['..','...','....','``',\"''\", '//n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OOkzDwiLgW07"
   },
   "outputs": [],
   "source": [
    "jepDict = json.load(open(DIRNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided that the questions that are considered \"hard\" are going to be the higher value questions. The next cell cleans the \"value\" category to remove the \"$\" character and the commas on 1,000 and 2,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8NOfDNTalZNl"
   },
   "outputs": [],
   "source": [
    "for entry in jepDict:\n",
    "  values = entry.get('value', '')\n",
    "  if values:\n",
    "    cleanVals = values.replace(',','').replace('$', '')\n",
    "    value = int(cleanVals)\n",
    "  elif values == 'None':\n",
    "    value = 0\n",
    "  entry['value'] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqt5tLbUeYYS"
   },
   "source": [
    "Here I am defining a function that will collect just the question and the answer then lemmatize and tokenize those words and exclude the ones that are stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "P-8KDR-leY5K"
   },
   "outputs": [],
   "source": [
    "def clean_wordlist(jsonList):\n",
    "  '''\n",
    "  Takes a list of dictionaries, and provides\n",
    "  a word list for all the questions and answers, lemmatized wihout stopwwords.\n",
    "  '''\n",
    "  corpus = ''\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for dictionary in jsonList:\n",
    "    question = dictionary.get('question').lower()\n",
    "    answer = dictionary.get('answer').lower()\n",
    "    corpus += '\\n'.join([question, answer])\n",
    "  wordlist = [lemmatizer.lemmatize(x) for x in word_tokenize(corpus) if x not in english_stopwords]\n",
    "  return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FAUxpSfHuRe",
    "outputId": "7671738e-baaf-4e0f-d0a1-425e74aad663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":11:29\n",
      ":13:44\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(':%M:%S'))\n",
    "jepWords = clean_wordlist(jepDict)\n",
    "print(datetime.now().strftime(':%M:%S')) #Just printing to see how long it takes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kept getting an error because my lists were not the same length. Turns out I need to classify each word in worlist first then I can add them to the dataframe. Thanks to chatGPT for helping me figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_text = [entry for entry in jepDict if entry['value'] > 600]\n",
    "hardWords = set(clean_wordlist(hard_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTSjdUgKwspe"
   },
   "source": [
    "Now to label the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ 1 if word in hardWords else 0 for word in jepWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "F8NyLQaKwuV8",
    "outputId": "14e105ce-c3ac-4088-ca90-c4599d15ce50"
   },
   "outputs": [],
   "source": [
    "df = pd.pandas.DataFrame({'text':jepWords, 'hard':labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "eHD_mhlo0oAp",
    "outputId": "6bd02f65-5998-476e-d727-99c5d13afb5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2280394</th>\n",
       "      <th>2280395</th>\n",
       "      <th>2280396</th>\n",
       "      <th>2280397</th>\n",
       "      <th>2280398</th>\n",
       "      <th>2280399</th>\n",
       "      <th>2280400</th>\n",
       "      <th>2280401</th>\n",
       "      <th>2280402</th>\n",
       "      <th>2280403</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>'for</td>\n",
       "      <td>last</td>\n",
       "      <td>8</td>\n",
       "      <td>year</td>\n",
       "      <td>life</td>\n",
       "      <td>galileo</td>\n",
       "      <td>house</td>\n",
       "      <td>arrest</td>\n",
       "      <td>espousing</td>\n",
       "      <td>man</td>\n",
       "      <td>...</td>\n",
       "      <td>name</td>\n",
       "      <td>18th</td>\n",
       "      <td>c.</td>\n",
       "      <td>statesman</td>\n",
       "      <td>favorite</td>\n",
       "      <td>catherine</td>\n",
       "      <td>great</td>\n",
       "      <td>grigori</td>\n",
       "      <td>alexandrovich</td>\n",
       "      <td>potemkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 2280404 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1       2       3       4        5       6       7        \\\n",
       "text    'for    last       8    year    life  galileo   house  arrest   \n",
       "hard       1       1       1       1       1        1       1       1   \n",
       "\n",
       "        8       9        ... 2280394 2280395 2280396    2280397   2280398  \\\n",
       "text  espousing     man  ...    name    18th      c.  statesman  favorite   \n",
       "hard          0       1  ...       1       1       1          1         1   \n",
       "\n",
       "        2280399 2280400  2280401        2280402   2280403  \n",
       "text  catherine   great  grigori  alexandrovich  potemkin  \n",
       "hard          1       1        1              1         1  \n",
       "\n",
       "[2 rows x 2280404 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF54mGHBWXTg"
   },
   "source": [
    "Now for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CiaiXd_9ZgyE"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score #This tells us how accurate we were with training our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['hard'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf = True)\n",
    "X_train_tf = tfidf_vectorizer.fit_transform(X_train) #chaging th tfidf data into trained vectors\n",
    "X_test_tf = tfidf_vectorizer.transform(X_test) #vectorizing it bt not the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_tf, y_train) #try to fit this using our training data\n",
    "prediction = naive_bayes.predict(X_test_tf) #predict the liklihood for the rest of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.946360522888428\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test, prediction)) #predicts the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a little bit high, and I felt like the best way to classify hard questions was to use the value that jeapordy already assigned to the question."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
